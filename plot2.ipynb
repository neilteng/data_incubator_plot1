{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from MortalityPrediction.src.ml_utils import read_and_extract_features, print_metrics_binary, Reader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"MortalityPrediction/data/preprocessed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and extracting features ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing the data to have zero mean and unit variance ...\n"
     ]
    }
   ],
   "source": [
    "#dataset reader\n",
    "train_reader = Reader(dataset_dir=DATA + \"train\", listfile=DATA + \"train_listfile.csv\")\n",
    "val_reader = Reader(dataset_dir=DATA + \"train\", listfile=DATA + \"val_listfile.csv\")\n",
    "test_reader = Reader(dataset_dir=DATA + \"test\", listfile=DATA + \"test_listfile.csv\")                \n",
    "print('Reading data and extracting features ...')\n",
    "(train_X, train_y) = read_and_extract_features(train_reader)\n",
    "(val_X, val_y) = read_and_extract_features(val_reader)\n",
    "(test_X, test_y) = read_and_extract_features(test_reader)\n",
    "print('Imputing missing values ...')\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean', verbose=0, copy=True)\n",
    "imputer.fit(train_X)\n",
    "train_X = np.array(imputer.transform(train_X), dtype=np.float32)\n",
    "val_X = np.array(imputer.transform(val_X), dtype=np.float32)\n",
    "test_X = np.array(imputer.transform(test_X), dtype=np.float32)\n",
    "print('Normalizing the data to have zero mean and unit variance ...')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)\n",
    "train_X = scaler.transform(train_X)\n",
    "val_X = scaler.transform(val_X)\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef_:  [[ 3.37299253e-03  1.11993325e-02  2.51913479e-03  1.57538515e-02\n   3.44756841e-03 -4.09424638e-03 -9.79094171e-03 -1.35300967e-03\n  -5.24794373e-03  2.77960175e-02 -7.32993336e-03 -1.19106652e-02\n  -3.39308511e-03 -2.06850480e-03 -1.62253533e-03 -1.78561541e-04\n  -8.03801117e-03 -9.60640747e-03  5.58016786e-03 -1.45475598e-03\n  -3.27842269e-03 -8.06867913e-03  9.65633504e-03 -8.15270638e-03\n   2.80288111e-03  2.80288111e-03  2.80288111e-03  0.00000000e+00\n   0.00000000e+00 -2.31222994e-03  2.70083470e-03  2.80288111e-03\n   2.77748572e-03  7.70452439e-04 -7.70454819e-04 -6.18040085e-03\n   7.31750511e-03  5.56722502e-03  3.78890949e-03 -1.72219402e-04\n   9.34765242e-03 -7.12878477e-03 -4.19469807e-03  9.00873562e-03\n  -1.85442766e-02  7.04086747e-03  1.44587575e-02 -2.72915891e-03\n  -3.49995701e-02  1.28529702e-02 -5.06999435e-03  5.33914260e-03\n   2.29388551e-02 -2.36049653e-02 -2.51435677e-02  1.02685966e-02\n  -8.39128787e-03  4.38327140e-03 -1.27278415e-02 -7.56885088e-03\n  -2.75069569e-02  7.54067099e-03 -1.55469867e-02  3.11772156e-03\n   1.11627220e-02 -2.88817007e-03 -3.13558672e-02 -3.97619754e-02\n  -4.92714371e-02 -2.53516030e-02 -1.73729507e-02  8.33995400e-03\n  -2.54233631e-02  1.54565617e-02  4.43580153e-03  1.58893070e-02\n   4.53495042e-03  2.20265022e-03 -1.11189566e-02  3.85042038e-03\n  -1.37863405e-02  4.62186080e-03  6.53233889e-03 -2.06640729e-03\n   6.86674282e-03  3.18302011e-03  8.25510157e-03 -2.52312297e-03\n   1.46569740e-03  4.03336050e-03 -2.62325077e-03 -3.44537353e-03\n  -1.35089862e-03  3.61002311e-04  3.18305435e-02  7.48836746e-03\n  -9.07586474e-03 -6.58684928e-03 -5.65449607e-03  2.42283341e-03\n  -2.37459531e-02  9.90552388e-03 -7.06839425e-03  5.69624001e-03\n  -2.61147597e-03  1.34700259e-02 -1.12135005e-03  5.79147061e-03\n   1.54518663e-02  1.10694016e-02  1.48928190e-02 -2.79269517e-03\n  -4.73944195e-03  2.83412340e-03  4.23372515e-03  2.72173134e-03\n   2.74281597e-03  5.47951016e-03  1.70688187e-03  2.87657827e-03\n   2.83773769e-02 -1.91330548e-03  2.13191559e-02 -2.48184221e-02\n  -1.91664269e-02  1.59746132e-03 -9.50184494e-03 -3.62552724e-02\n  -1.06169288e-02 -1.54897771e-03  1.34466120e-02  1.27657076e-02\n  -4.85422115e-03 -8.80479213e-03 -5.94599987e-03 -7.23625663e-03\n  -1.12791692e-02 -6.75799087e-03 -4.45496653e-03  4.09881664e-03\n  -2.49680600e-04  1.59636676e-02  2.67374476e-02  1.22306291e-02\n  -6.66147792e-03 -1.24343430e-02 -3.94189504e-03  8.51450556e-03\n  -1.41685143e-02  1.65813104e-02 -4.16200349e-02 -4.55173145e-02\n  -4.29751739e-02  6.80889292e-03 -9.05902823e-03 -8.46495698e-03\n  -3.57538105e-02 -2.62543063e-02 -2.84563687e-02  2.93719029e-02\n  -1.35353013e-02  7.93201162e-03 -1.82440916e-02 -3.08383108e-02\n  -2.21349738e-02  1.02997445e-02  8.61708383e-03  5.72197744e-03\n   1.42516365e-02 -3.89002439e-02 -1.12926204e-02 -3.98614843e-02\n   2.51184835e-03  1.13654352e-02  1.30827652e-02  3.25584964e-03\n   7.45776957e-03 -1.64754335e-02  8.14662750e-03 -1.33522395e-03\n   6.90297037e-03  6.24738554e-03  9.50960122e-03 -7.45287964e-03\n  -1.31540177e-02  1.05278064e-02  1.11991459e-02 -9.67603398e-03\n  -4.42464584e-03 -3.08975422e-02  1.15640258e-02  1.37224530e-02\n  -2.44904220e-02 -4.72132830e-02 -3.66098218e-02 -3.85174855e-02\n   1.33828393e-02 -9.13145394e-03 -1.55392141e-02 -4.57370162e-02\n  -2.90712617e-02 -3.18487688e-02 -1.74550433e-03  1.01417495e-02\n  -1.28267297e-02 -3.12374596e-02 -2.87140160e-02 -4.53433251e-03\n   1.12249309e-02  5.36213785e-03  8.76304451e-03 -8.01650522e-03\n  -6.43359989e-03 -2.79488755e-02  9.32740177e-03  6.24198202e-03\n   1.49953985e-02  2.20641462e-02  1.86289482e-02  3.38143389e-03\n  -1.93739607e-02 -2.10374111e-02  6.87305913e-03  7.68813373e-03\n   9.07233996e-03  1.30585700e-04  1.19864136e-03 -5.56971711e-03\n   3.40803218e-03 -5.98931764e-03 -2.39060870e-03 -1.93948061e-02\n  -3.72624825e-03 -6.64318515e-04 -1.56856840e-02 -1.91635574e-02\n  -1.85113040e-02 -1.15298343e-03 -1.36448362e-03 -3.30001546e-03\n  -1.38051171e-02 -1.64422130e-02 -1.53922844e-02 -1.53953078e-04\n  -1.75872517e-02  2.67719992e-02 -1.80033993e-02 -1.32285056e-02\n  -1.83898278e-02  1.59971931e-02  5.12113187e-03  1.12774220e-02\n  -1.60344258e-02 -3.95288922e-02 -2.29578586e-02 -2.91338604e-02\n   4.16684206e-03  1.45998398e-02  1.66059637e-02  1.51472497e-02\n   1.48352019e-02 -2.98521326e-03 -9.21925843e-03 -4.54388624e-03\n   4.15179938e-03  1.35043137e-02  9.43511936e-03  1.39706155e-02\n  -1.33916830e-02  1.35804115e-02 -1.37060773e-03 -6.30607010e-03\n  -5.40332788e-03 -6.16468500e-03 -1.46298799e-03  1.72591581e-02\n  -6.04108148e-02 -5.43772009e-02 -5.85784058e-02  2.27651561e-02\n  -5.86707630e-04 -4.63802143e-03 -5.25393518e-02 -4.83674421e-02\n  -4.88582422e-02  1.14926412e-02 -1.20478709e-02  1.06512685e-02\n  -4.85185536e-02 -5.91823004e-02 -4.51365245e-02 -4.09644474e-03\n  -4.28064491e-02  7.98832817e-03 -1.66205625e-02 -2.97226076e-03\n  -5.23202482e-04 -2.46449935e-03 -8.72690497e-03 -2.98374722e-02\n  -4.93215910e-03 -1.33718844e-02 -1.98152683e-03 -2.76477011e-03\n  -2.61345434e-02 -4.84021909e-02  1.80431547e-02 -1.13311225e-02\n   1.42577822e-02 -2.19960680e-04 -5.61425977e-03 -4.77081123e-02\n   1.22466091e-02 -3.03788566e-03 -1.57091571e-03 -2.76606633e-03\n   1.59025116e-02 -5.02939891e-02  9.57991180e-03  1.08017487e-02\n   6.89925808e-03 -1.45081172e-03  1.67047886e-02  1.56823521e-02\n   9.22625616e-03  1.23069591e-02  1.26209842e-02  6.05038393e-03\n   2.66772073e-03  1.50955936e-02  4.18851422e-02  1.11211663e-02\n   4.16743372e-02 -6.49716629e-03 -1.39931170e-02  6.43259619e-03\n  -1.70392508e-02 -2.72314743e-03  1.67805789e-02  1.96045161e-04\n   4.95062223e-02  1.97668022e-03  2.41211451e-03 -1.40583661e-02\n   4.95281925e-03 -2.43000280e-02 -2.27829995e-02 -5.21172856e-03\n   6.64974170e-03  1.00766679e-02  1.84001723e-02  8.05466423e-03\n   2.31815887e-02  1.93494874e-03  6.21653561e-03  7.44468532e-03\n   8.19751881e-03  1.26693103e-02  3.50898310e-02  5.44228018e-03\n   2.81473749e-02  3.06819918e-02  4.10654458e-02  3.54406965e-03\n  -3.43978275e-02  6.58131801e-03  1.73545889e-02 -5.96275469e-03\n   2.97473020e-02 -1.42031294e-02 -2.73979486e-02  2.08954742e-03\n   1.42811370e-02 -3.26272261e-03  2.68864023e-02 -1.31298858e-02\n   3.63485353e-03 -1.43883283e-03 -1.05439569e-03 -1.05439569e-03\n  -1.05439569e-03  0.00000000e+00  0.00000000e+00 -2.16869652e-02\n  -1.05439569e-03 -1.05439569e-03 -1.05439569e-03  0.00000000e+00\n   0.00000000e+00  0.00000000e+00 -1.05439569e-03 -1.05439569e-03\n  -1.05439569e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  -1.05439569e-03 -1.05439569e-03 -1.05439569e-03  0.00000000e+00\n   0.00000000e+00  0.00000000e+00 -1.05439569e-03 -1.05439569e-03\n  -1.05439569e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  -1.05439569e-03 -1.05439569e-03 -1.05439569e-03  0.00000000e+00\n   0.00000000e+00  0.00000000e+00 -1.05439569e-03 -1.05439569e-03\n  -1.05439569e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  -1.44238944e-02  7.47998205e-04 -1.65576678e-02  1.29097462e-03\n   1.82959161e-03 -2.06802144e-03 -2.24267202e-02 -6.09529568e-03\n  -2.70384676e-02  4.67335539e-03  2.32914836e-02 -2.31567498e-02\n   2.32163363e-03 -6.81887658e-03 -1.55636688e-02 -1.76284724e-03\n  -2.38588189e-03 -6.81647156e-03 -2.28506096e-02  8.23761064e-03\n  -1.82711303e-02  1.71732140e-02 -2.38810709e-02 -2.21117519e-03\n  -4.79393811e-03 -4.09427405e-03 -9.08944977e-03 -4.34533480e-03\n   7.90269261e-03  7.84045091e-03 -5.85069488e-03  1.80917124e-03\n  -1.50985370e-02  4.18273890e-03  3.59428306e-03  3.27452513e-03\n  -1.03481162e-02 -3.17883407e-04 -6.29911479e-03 -6.29127853e-04\n  -1.39417433e-03 -1.53949159e-03 -2.78537197e-02 -1.16080502e-03\n  -1.91735825e-03 -1.05128519e-03 -3.05857542e-03  3.53295250e-03\n   1.37099537e-02  2.07265581e-04 -4.95824720e-03 -7.41975054e-03\n   1.91102543e-02 -1.30051409e-02 -2.30723940e-02 -1.13665937e-03\n  -1.18230141e-03 -1.03832130e-03 -1.23643315e-02  5.51653165e-04\n  -3.92732755e-03 -1.13992130e-03 -1.24683837e-03 -1.08607348e-03\n   6.36342221e-03  7.09013200e-03 -4.27992474e-02 -1.47620175e-03\n  -1.86294553e-02  1.69784603e-03  1.14370014e-02  1.53500881e-02\n  -4.63119939e-02 -2.19506995e-03 -3.56801344e-02  2.53022355e-03\n   2.09169331e-02  8.26051918e-03 -4.53947452e-02 -1.78754197e-03\n  -4.88655734e-02  4.34320534e-03  1.65042121e-02 -2.10485265e-04\n   2.61645755e-02 -9.63393651e-04  1.98378273e-04 -9.47199113e-04\n  -1.83848918e-02  3.94459745e-03  5.20477090e-02 -9.38632984e-04\n  -7.65334297e-04 -9.54227534e-04 -2.79179544e-02 -7.76056435e-03\n   2.73059884e-02 -9.34926048e-04 -5.86792336e-04 -9.36077321e-04\n  -2.46287104e-02  1.39308296e-03  8.42601681e-03 -9.47855176e-04\n  -3.64218673e-04 -9.39342206e-04 -2.00824825e-02  6.84202090e-03\n   5.26366657e-03  3.95064578e-02  3.72894244e-02  4.14682706e-02\n   2.38583231e-03  3.07178322e-03  1.08349473e-03  1.08039439e-02\n   3.60584003e-02  2.44079113e-02 -3.13853587e-02  3.06026269e-03\n  -4.34010788e-03  1.31554689e-02  4.87957051e-02  3.05619429e-02\n  -8.76796918e-03  8.50074092e-04 -1.03251083e-02  2.88203373e-03\n  -1.25605467e-02  3.47918019e-03 -1.72892371e-02 -2.73672975e-03\n   4.81831523e-04  2.38150269e-03 -8.10001194e-03  5.26006158e-03\n   1.04037624e-02 -2.34971277e-02  1.44818936e-02 -3.64997154e-03\n  -2.46002862e-03 -4.07285222e-03 -1.64903666e-02 -7.53141607e-03\n   2.47484422e-03 -8.54914159e-03 -9.83830327e-03 -6.92663898e-03\n  -1.72579478e-02 -2.86410459e-03 -2.81047966e-02  5.40817678e-03\n  -2.19626979e-02  9.17975815e-03 -5.82201822e-04  8.38048401e-03\n  -1.81176949e-02  5.69262789e-03 -1.85931090e-02  8.55306717e-03\n  -9.03418265e-03  2.16147886e-03 -1.01197641e-02  4.87608416e-03\n  -7.16913770e-03 -7.13428745e-05 -2.99036331e-02 -2.13544253e-03\n   2.23187392e-02  9.95643496e-04 -3.53080823e-02  1.70813291e-04\n   4.35948488e-02 -2.70307818e-03 -1.09146339e-02 -3.55840730e-02\n  -3.36947129e-02 -1.51548278e-02 -1.68131398e-02 -1.46027308e-02\n   8.19866039e-03 -5.22543644e-02 -4.54731354e-02 -4.03385188e-02\n  -2.12387702e-02 -5.92588477e-03 -2.96121705e-02  1.64544839e-02\n  -8.14879358e-02  2.90964722e-02 -1.11505417e-02 -5.39954911e-03\n  -5.64042621e-02 -3.69797163e-02 -5.33911879e-02 -2.39414624e-02\n   3.34319005e-03 -2.43725526e-02 -3.89924330e-02 -3.25851407e-03\n  -1.78682992e-02 -2.43212855e-03  2.15652855e-02  7.51732903e-03\n  -3.19511544e-02 -3.64998552e-03 -2.89636920e-02 -2.70104824e-03\n   2.92356406e-03  8.11853786e-03 -1.31918183e-02 -3.97592844e-03\n  -7.08747725e-03  1.62639165e-02 -3.38304748e-03  5.56286689e-03\n   2.19474857e-03  2.71919435e-03  2.25917258e-03  8.33930982e-03\n   1.34760539e-02 -2.11153923e-02  1.70485420e-03  1.20787955e-03\n   1.15974843e-03  3.32020997e-04  1.23458645e-02 -9.68004336e-03\n   4.71035033e-03 -5.39032335e-03  8.09046613e-04 -2.62693005e-02\n  -2.29792663e-02  1.07937461e-02 -1.73616963e-02 -1.86776173e-02\n  -1.82642388e-02 -6.74301875e-03 -2.69064346e-02  3.34425718e-03\n  -1.58617177e-02 -1.47465040e-02 -1.54568642e-02  1.53260175e-03\n  -2.87665645e-03  1.94319615e-02 -1.62295502e-02 -1.44098604e-02\n  -1.51578988e-02  2.79626927e-03 -5.21127482e-03  4.32321262e-02\n  -1.41829263e-02 -5.26609964e-03 -5.72574310e-03  2.29087053e-02\n  -2.62446274e-02 -2.77360475e-02  3.17429628e-03  4.36503983e-03\n   2.03102065e-03  1.14149904e-02  6.61463112e-03 -6.01159123e-02\n   3.92000689e-03  2.83845639e-03  5.05051925e-04  1.34392219e-02\n  -8.32797148e-03 -5.65979676e-02 -1.06707537e-02  1.12864950e-03\n  -6.50812413e-03  5.86367895e-02  1.28630746e-03 -5.37912992e-02\n  -5.78471626e-03 -7.42636286e-03 -5.97239752e-03 -1.04275555e-02\n   1.04204632e-02 -3.88672612e-03 -2.24106063e-03 -4.63554383e-03\n  -2.72225972e-03 -1.43272660e-02 -2.02198094e-02 -2.65778711e-03\n   1.02227183e-03  1.07184833e-03 -1.29747478e-03  4.41630068e-03\n   2.14843578e-02  1.14930870e-02]]\n\nResults on train set\nconfusion matrix:\n[[12420   274]\n [ 1419   568]]\naccuracy = 0.8846808671951294\nprecision class 0 = 0.8974636793136597\nprecision class 1 = 0.6745843291282654\nrecall class 0 = 0.9784150123596191\nrecall class 1 = 0.28585806488990784\nAUC of ROC = 0.8632204333683359\nAUC of PRC = 0.5507722714423886\n\nResults on eval set\nconfusion matrix:\n[[2707   79]\n [ 315  121]]\naccuracy = 0.8777157068252563\nprecision class 0 = 0.8957644104957581\nprecision class 1 = 0.6050000190734863\nrecall class 0 = 0.9716439247131348\nrecall class 1 = 0.2775229215621948\nAUC of ROC = 0.8438901585252606\nAUC of PRC = 0.4924434938097143\n\nResults on test set\nconfusion matrix:\n[[2798   64]\n [ 256  118]]\naccuracy = 0.9011124968528748\nprecision class 0 = 0.9161754846572876\nprecision class 1 = 0.6483516693115234\nrecall class 0 = 0.9776380062103271\nrecall class 1 = 0.31550800800323486\nAUC of ROC = 0.848530626277574\nAUC of PRC = 0.4823380792491313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9011125,\n 'auprc': 0.4823380792491313,\n 'auroc': 0.848530626277574,\n 'prec0': 0.9161755,\n 'prec1': 0.64835167,\n 'rec0': 0.977638,\n 'rec1': 0.315508}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty=\"l2\", C=0.001, solver='lbfgs')\n",
    "clf.fit(train_X, train_y)\n",
    "print(\"coef_: \", clf.coef_)\n",
    "#print result in terminal\n",
    "print('\\nResults on train set')\n",
    "print_metrics_binary(train_y, clf.predict_proba(train_X))\n",
    "print('\\nResults on eval set')\n",
    "print_metrics_binary(val_y, clf.predict_proba(val_X))\n",
    "prediction = clf.predict_proba(test_X)[:, 1]\n",
    "print('\\nResults on test set')\n",
    "print_metrics_binary(test_y, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nResults on train set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n[[12539   155]\n [ 1373   614]]\naccuracy = 0.895919919013977\nprecision class 0 = 0.9013082385063171\nprecision class 1 = 0.7984395027160645\nrecall class 0 = 0.987789511680603\nrecall class 1 = 0.30900856852531433\nAUC of ROC = 0.9277144039058354\nAUC of PRC = 0.6872543865092984\n\nResults on eval set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n[[2716   70]\n [ 355   81]]\naccuracy = 0.8680943250656128\nprecision class 0 = 0.8844024538993835\nprecision class 1 = 0.5364238619804382\nrecall class 0 = 0.9748743772506714\nrecall class 1 = 0.18577980995178223\nAUC of ROC = 0.7125116078426208\nAUC of PRC = 0.3764556193816045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nResults on test set\nconfusion matrix:\n[[2793   69]\n [ 308   66]]\naccuracy = 0.8834981322288513\nprecision class 0 = 0.9006772041320801\nprecision class 1 = 0.4888888895511627\nrecall class 0 = 0.9758909940719604\nrecall class 1 = 0.1764705926179886\nAUC of ROC = 0.7122108992253275\nAUC of PRC = 0.35072461277637296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.88349813,\n 'auprc': 0.35072461277637296,\n 'auroc': 0.7122108992253275,\n 'prec0': 0.9006772,\n 'prec1': 0.4888889,\n 'rec0': 0.975891,\n 'rec1': 0.1764706}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(train_X, train_y)\n",
    "#print result in terminal\n",
    "print('\\nResults on train set')\n",
    "print_metrics_binary(train_y, clf.predict_proba(train_X))\n",
    "print('\\nResults on eval set')\n",
    "print_metrics_binary(val_y, clf.predict_proba(val_X))\n",
    "prediction = clf.predict_proba(test_X)[:, 1]\n",
    "print('\\nResults on test set')\n",
    "print_metrics_binary(test_y, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karlie/PycharmProjects/3.7/venv/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nResults on train set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n[[12605    89]\n [ 1130   857]]\naccuracy = 0.916967511177063\nprecision class 0 = 0.9177284240722656\nprecision class 1 = 0.9059196710586548\nrecall class 0 = 0.9929888248443604\nrecall class 1 = 0.431303471326828\nAUC of ROC = 0.959215838827596\nAUC of PRC = 0.8483804599892494\n\nResults on eval set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n[[2719   67]\n [ 322  114]]\naccuracy = 0.8792675137519836\nprecision class 0 = 0.8941137790679932\nprecision class 1 = 0.6298342347145081\nrecall class 0 = 0.9759511947631836\nrecall class 1 = 0.26146790385246277\nAUC of ROC = 0.8245334635167977\nAUC of PRC = 0.5040367460352914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nResults on test set\nconfusion matrix:\n[[2807   55]\n [ 267  107]]\naccuracy = 0.9004944562911987\nprecision class 0 = 0.9131425023078918\nprecision class 1 = 0.6604938507080078\nrecall class 0 = 0.980782687664032\nrecall class 1 = 0.2860962450504303\nAUC of ROC = 0.821703438379354\nAUC of PRC = 0.476884175815348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.90049446,\n 'auprc': 0.476884175815348,\n 'auroc': 0.821703438379354,\n 'prec0': 0.9131425,\n 'prec1': 0.66049385,\n 'rec0': 0.9807827,\n 'rec1': 0.28609625}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(C=1.0, kernel=\"rbf\", probability=True)\n",
    "clf.fit(train_X, train_y)\n",
    "#print result in terminal\n",
    "print('\\nResults on train set')\n",
    "print_metrics_binary(train_y, clf.predict_proba(train_X))\n",
    "print('\\nResults on eval set')\n",
    "print_metrics_binary(val_y, clf.predict_proba(val_X))\n",
    "prediction = clf.predict_proba(test_X)[:, 1]\n",
    "print('\\nResults on test set')\n",
    "print_metrics_binary(test_y, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nResults on train set\nconfusion matrix:\n[[12694     0]\n [    0  1987]]\naccuracy = 1.0\nprecision class 0 = 1.0\nprecision class 1 = 1.0\nrecall class 0 = 1.0\nrecall class 1 = 1.0\nAUC of ROC = 1.0\nAUC of PRC = 1.0\n\nResults on eval set\nconfusion matrix:\n[[2466  320]\n [ 272  164]]\naccuracy = 0.8162631988525391\nprecision class 0 = 0.9006574153900146\nprecision class 1 = 0.3388429880142212\nrecall class 0 = 0.8851400017738342\nrecall class 1 = 0.3761467933654785\nAUC of ROC = 0.6306433873166619\nAUC of PRC = 0.3997046896716547\n\nResults on test set\nconfusion matrix:\n[[2548  314]\n [ 236  138]]\naccuracy = 0.8300370573997498\nprecision class 0 = 0.915229856967926\nprecision class 1 = 0.3053097426891327\nrecall class 0 = 0.8902865052223206\nrecall class 1 = 0.3689839541912079\nAUC of ROC = 0.6296352350736368\nAUC of PRC = 0.37361161718888336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.83003706,\n 'auprc': 0.37361161718888336,\n 'auroc': 0.6296352350736368,\n 'prec0': 0.91522986,\n 'prec1': 0.30530974,\n 'rec0': 0.8902865,\n 'rec1': 0.36898395}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "clf.fit(train_X, train_y)\n",
    "#print result in terminal\n",
    "print('\\nResults on train set')\n",
    "print_metrics_binary(train_y, clf.predict_proba(train_X))\n",
    "print('\\nResults on eval set')\n",
    "print_metrics_binary(val_y, clf.predict_proba(val_X))\n",
    "prediction = clf.predict_proba(test_X)[:, 1]\n",
    "print('\\nResults on test set')\n",
    "print_metrics_binary(test_y, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nResults on train set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n[[12694     0]\n [    7  1980]]\naccuracy = 0.9995232224464417\nprecision class 0 = 0.999448835849762\nprecision class 1 = 1.0\nrecall class 0 = 1.0\nrecall class 1 = 0.9964771270751953\nAUC of ROC = 1.0\nAUC of PRC = 1.0\n\nResults on eval set\nconfusion matrix:\n[[2738   48]\n [ 318  118]]\naccuracy = 0.8864059448242188\nprecision class 0 = 0.8959423899650574\nprecision class 1 = 0.7108433842658997\nrecall class 0 = 0.9827709794044495\nrecall class 1 = 0.2706421911716461\nAUC of ROC = 0.8439342024671193\nAUC of PRC = 0.5397045091170963\n\nResults on test set\nconfusion matrix:\n[[2813   49]\n [ 284   90]]\naccuracy = 0.8970952033996582\nprecision class 0 = 0.9082983732223511\nprecision class 1 = 0.6474820375442505\nrecall class 0 = 0.982879102230072\nrecall class 1 = 0.24064171314239502\nAUC of ROC = 0.8415028008535221\nAUC of PRC = 0.48067137423321316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.8970952,\n 'auprc': 0.48067137423321316,\n 'auroc': 0.8415028008535221,\n 'prec0': 0.9082984,\n 'prec1': 0.64748204,\n 'rec0': 0.9828791,\n 'rec1': 0.24064171}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2)\n",
    "clf.fit(train_X, train_y)\n",
    "#print result in terminal\n",
    "print('\\nResults on train set')\n",
    "print_metrics_binary(train_y, clf.predict_proba(train_X))\n",
    "print('\\nResults on eval set')\n",
    "print_metrics_binary(val_y, clf.predict_proba(val_X))\n",
    "prediction = clf.predict_proba(test_X)[:, 1]\n",
    "print('\\nResults on test set')\n",
    "print_metrics_binary(test_y, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nResults on train set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n[[12362   332]\n [ 1362   625]]\naccuracy = 0.8846127390861511\nprecision class 0 = 0.9007577896118164\nprecision class 1 = 0.653082549571991\nrecall class 0 = 0.973845899105072\nrecall class 1 = 0.31454452872276306\nAUC of ROC = 0.8782041121393357\nAUC of PRC = 0.5568575675871068\n\nResults on eval set\nconfusion matrix:\n[[2702   84]\n [ 299  137]]\naccuracy = 0.8811297416687012\nprecision class 0 = 0.9003665447235107\nprecision class 1 = 0.6199095249176025\nrecall class 0 = 0.9698492288589478\nrecall class 1 = 0.3142201900482178\nAUC of ROC = 0.8544039002351207\nAUC of PRC = 0.524463321878077\n\nResults on test set\nconfusion matrix:\n[[2784   78]\n [ 260  114]]\naccuracy = 0.8955500721931458\nprecision class 0 = 0.914586067199707\nprecision class 1 = 0.59375\nrecall class 0 = 0.9727463126182556\nrecall class 1 = 0.30481284856796265\nAUC of ROC = 0.8522484370153627\nAUC of PRC = 0.48666948629288304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.8955501,\n 'auprc': 0.48666948629288304,\n 'auroc': 0.8522484370153627,\n 'prec0': 0.91458607,\n 'prec1': 0.59375,\n 'rec0': 0.9727463,\n 'rec1': 0.30481285}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=50, learning_rate=0.5)\n",
    "clf.fit(train_X, train_y)\n",
    "#print result in terminal\n",
    "print('\\nResults on train set')\n",
    "print_metrics_binary(train_y, clf.predict_proba(train_X))\n",
    "print('\\nResults on eval set')\n",
    "print_metrics_binary(val_y, clf.predict_proba(val_X))\n",
    "prediction = clf.predict_proba(test_X)[:, 1]\n",
    "print('\\nResults on test set')\n",
    "print_metrics_binary(test_y, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
